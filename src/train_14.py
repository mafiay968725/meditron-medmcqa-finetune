from pathlib import Path
import torch
from transformers import BitsAndBytesConfig
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.optim import AdamW
from torch.utils.data import DataLoader
from peft import LoraConfig, get_peft_model, TaskType
import os
from peft import PeftModel
from datasets import load_from_disk
from torch.utils.data._utils.collate import default_collate
import torch.nn.functional as F
import torch.nn as nn
import csv
from pathlib import Path

from src.train_12 import save_path


def train_model(lora_rank=8, dropout=0.1, learning_rate=1e-4):


    # ç¯å¢ƒå˜é‡ä¼˜åŒ– CUDA æ˜¾å­˜
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64"
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

    # âœ… ç”¨ pathlib æŒ‡å®šæœ¬åœ°æ¨¡å‹è·¯å¾„
    base_dir = Path("/home/ubuntu/meditron-medmcqa-finetune")  # ä¿®æ”¹ä¸ºä½ çš„é¡¹ç›®æ ¹ç›®å½•
    model_path = base_dir / "models" / "meditron-7b"

    # 1) åŠ è½½ Tokenizerï¼ˆæœ¬åœ°ï¼‰
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        padding_side="left",
        local_files_only=True
    )
    tokenizer.pad_token = tokenizer.eos_token  # é¿å…å‡ºç°è­¦å‘Š

    # 2) 8-bit é‡åŒ–é…ç½®
    bnb_config = BitsAndBytesConfig(
        load_in_8bit=True,
        llm_int8_threshold=6.0,
        llm_int8_has_fp16_weight=False,
    )

    # 3) åŠ è½½æ¨¡å‹ï¼ˆæœ¬åœ°ï¼‰
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=bnb_config,
        device_map="auto",
        local_files_only=True  # åŠ è¿™å¥ï¼
    )


    #4) åŠ è½½æ•°æ®
    processed_data = load_from_disk("/home/ubuntu/meditron-medmcqa-finetune/data/processed_dataset_deepseek")
    # é‡Œé¢åŒ…å« train/dev/test åˆ†å‰²
    train_dataset = processed_data["train"]
    dev_dataset = processed_data["dev"]

    def format_example(example):
        # å¦‚æœä½ ä¹‹å‰åœ¨ "prompt" å­—æ®µå·²ç»åŒ…å«äº† "Answer: ???"
        # å¹¶ä¸” "label" æ˜¯ "A/B/C/D"
        # è¿™é‡Œç›´æ¥æŠŠå®ƒæ‹¼åˆ° prompt åé¢å³å¯
        text = example["prompt"] + " " + example["label"]  # ä¾‹å¦‚: "...Answer: C"
        return {"input_text": text}

    # 5) æ„å»ºLoraæ¨¡å‹
    lora_config = LoraConfig(
        r=lora_rank,
        lora_alpha=2*lora_rank,
        lora_dropout=dropout,
        task_type=TaskType.CAUSAL_LM,
    )
    model = get_peft_model(model, lora_config)
    model.to("cuda")  # ç¡®ä¿æ¨¡å‹åœ¨GPUä¸Š

    # 6) æ•°æ®åŠ è½½å™¨
    train_dataset = train_dataset.map(format_example)
    dev_dataset = dev_dataset.map(format_example)
    train_dataset = train_dataset.filter(lambda x: x is not None and "input_text" in x)
    dev_dataset = dev_dataset.filter(lambda x: x is not None and "input_text" in x)
    train_subset = train_dataset.shuffle(seed=42).select(range(10000))
    dev_subset = dev_dataset.shuffle(seed=42).select(range(1000))  # å…ˆç”¨éªŒè¯é›†çš„ä¸€éƒ¨åˆ†è¿›è¡Œè®¡ç®—


    def my_collate_fn(batch):
        for sample in batch:
            if sample is not None:
                # å°† None æ›¿æ¢ä¸ºé»˜è®¤å€¼ï¼Œä¾‹å¦‚ç©ºå­—ç¬¦ä¸²
                if sample.get("topic_name") is None:
                    sample["topic_name"] = ""
                if sample.get("exp") is None:
                    sample["exp"] = ""
        # è¿‡æ»¤æ‰æ•´ä½“ä¸º None çš„æ ·æœ¬
        filtered_batch = [sample for sample in batch if sample is not None]
        if len(filtered_batch) == 0:
            raise ValueError("è¿‡æ»¤åï¼Œå½“å‰æ‰¹æ¬¡æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬ï¼Œè¯·æ£€æŸ¥æ•°æ®é¢„å¤„ç†é€»è¾‘")
        return torch.utils.data.dataloader.default_collate(filtered_batch)

    train_dataloader = DataLoader(train_subset, batch_size=3, shuffle=True, collate_fn=my_collate_fn)
    dev_subset_dataloader = DataLoader(train_subset, batch_size=3, shuffle=True, collate_fn=my_collate_fn)
    # 7) ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    optimizer = AdamW(model.parameters(), lr=learning_rate)

    # 8) è®­ç»ƒå¾ªç¯
    epochs = 3
    accumulation_steps = 5  # æ¯5ä¸ªmini-batchç´¯ç§¯ä¸€æ¬¡æ¢¯åº¦
    global_step = 0  # è®°å½•çœŸå®çš„ä¼˜åŒ–æ­¥æ•°ï¼ˆæ¯å®Œæˆä¸€æ¬¡optimizer.step()å°±+1ï¼‰

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()  # ç¡®ä¿æ¯ä¸ªepochå¼€å§‹æ—¶æ¢¯åº¦ä¸º0

        for i, batch in enumerate(train_dataloader):
            # 1. å‡†å¤‡è¾“å…¥
            if "input_text" in batch:
                inputs = tokenizer(batch["input_text"], return_tensors="pt",
                                   padding=True, truncation=True, max_length=896).to("cuda")
            else:
                continue
            # 2. å‰å‘ä¼ æ’­
            labels = inputs.input_ids
            outputs = model(**inputs, labels=labels)
            loss = outputs.loss

            # 3. æ¢¯åº¦ç´¯ç§¯: æ¯æ¬¡åªåä¼  loss / accumulation_steps
            (loss / accumulation_steps).backward()

            # 4. ç´¯è®¡åˆ°ä¸€å®šæ­¥æ•°ï¼Œå†æ‰§è¡Œä¸€æ¬¡ä¼˜åŒ–
            if (i + 1) % accumulation_steps == 0:
                optimizer.step()
                optimizer.zero_grad()
                global_step += 1
            #5. å®šæ—¶æ¸…ç†æ˜¾å­˜ï¼Œé˜²æ­¢OOM
            if global_step % 300 == 0:
                torch.cuda.empty_cache()
            # 5. æ¯ eval_interval ä¸ª "ä¼˜åŒ–æ­¥" è¿›è¡Œä¸€æ¬¡è¯„ä¼°
            if global_step % 100 == 0:
                torch.cuda.empty_cache()

        #å½“å‰epochç»“æŸåï¼Œè¯„ä¼°dev_loss,å¹¶ä¿å­˜åˆ°csvæ–‡ä»¶ä¸­
        model.eval()
        total_loss = 0
        with torch.no_grad():
            for dev_batch in dev_subset_dataloader:
                dev_inputs = tokenizer(dev_batch["input_text"],
                                        return_tensors="pt", padding=True, truncation=True, max_length=896).to("cuda")
                dev_labels = dev_inputs.input_ids
                dev_outputs = model(**dev_inputs, labels=dev_labels)
                total_loss += dev_outputs.loss.item()
        avg_loss = total_loss / len(dev_subset_dataloader)
        print(f"Epoch {epoch + 1},  Dev Loss: {avg_loss:.4f}")
        save_path = "/home/ubuntu/meditron-medmcqa-finetune/data/log/train_13.csv"
        log_dev_loss_to_csv(epoch+1, lora_rank, dropout, learning_rate, avg_loss, save_path)
        model.train()



    #è¯„ä¼°å‡†ç¡®ç‡
    # 1. è‡ªå®šä¹‰ collate_fn
    def dev_collate_fn(batch):
        for sample in batch:
            if sample is not None:
                # å°† None æ›¿æ¢ä¸ºé»˜è®¤å€¼ï¼Œä¾‹å¦‚ç©ºå­—ç¬¦ä¸²
                if sample.get("topic_name") is None:
                    sample["topic_name"] = ""
                if sample.get("exp") is None:
                    sample["exp"] = ""
        # å°†æ¯ä¸ªæ ·æœ¬çš„ prompt å’Œ label åˆ†åˆ«æ”¶é›†åˆ°åˆ—è¡¨ä¸­
        prompts = [sample["prompt"] for sample in batch]
        gold_labels = [sample["label"] for sample in batch]
        return {"prompts": prompts, "gold_labels": gold_labels}

    # 2. æ„é€  DataLoaderï¼ˆå¯ä»¥æ ¹æ®ä½ çš„ç¡¬ä»¶é€‚å½“è°ƒæ•´ batch_sizeï¼‰
    dev_loader = DataLoader(dev_subset, batch_size=2, shuffle=False, collate_fn=dev_collate_fn)

    # 3. å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œæ‰¹é‡è®¡ç®—æ¯ä¸ªæ–‡æœ¬çš„ per-example loss
    def compute_per_example_loss(model, tokenizer, texts):
        """
        texts: list of stringï¼Œæ¯ä¸ªæ–‡æœ¬ä¸º "prompt + ' ' + option"
        è¿”å›ä¸€ä¸ª tensorï¼Œå½¢çŠ¶ä¸º (len(texts),)ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå¯¹åº”æ–‡æœ¬çš„ loss
        """
        inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=768)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        # å‰å‘ä¼ æ’­å¾—åˆ° logits
        outputs = model(**inputs, labels=inputs["input_ids"])
        logits = outputs.logits  # shape: (B, L, vocab_size)
        labels = inputs["input_ids"]  # shape: (B, L)

        # ä¸ºå› æœè¯­è¨€æ¨¡å‹åš shiftï¼ˆæ³¨æ„ï¼šshiftåä¸¤è€…é•¿åº¦å¯¹é½ï¼‰
        shift_logits = logits[:, :-1, :].contiguous()
        shift_labels = labels[:, 1:].contiguous()

        # è®¡ç®—æ¯ä¸ª token çš„ lossï¼Œä¸ä½œ reduction
        loss_fct = nn.CrossEntropyLoss(reduction="none")
        B, L, V = shift_logits.shape
        loss_tokens = loss_fct(shift_logits.view(-1, V), shift_labels.view(-1))
        loss_tokens = loss_tokens.view(B, L)

        # åˆ›å»º attention maskï¼šé pad çš„ä½ç½®ä¸º 1
        attention_mask = (shift_labels != tokenizer.pad_token_id).float()
        # æ¯ä¸ªæ ·æœ¬çš„ lossï¼šå¯¹æœ‰æ•ˆ token çš„ loss æ±‚å’Œåå½’ä¸€åŒ–
        per_example_loss = (loss_tokens * attention_mask).sum(dim=1) / (attention_mask.sum(dim=1) + 1e-8)
        return per_example_loss  # shape: (B,)

    # 4. å®šä¹‰è¯„ä¼°å‡½æ•°ï¼Œåˆ©ç”¨æ‰¹é‡è¯„ä¼°è®¡ç®—å‡†ç¡®ç‡
    def evaluate_accuracy_batch(model, tokenizer, dev_loader):
        options = ["A", "B", "C", "D"]
        total = 0
        correct = 0
        model.eval()
        with torch.no_grad():
            for batch in dev_loader:
                prompts = batch["prompts"]  # list of str, é•¿åº¦ä¸º batch_size
                gold_labels = batch["gold_labels"]  # list of str, é•¿åº¦ä¸º batch_size
                batch_size = len(prompts)

                # å¯¹æ¯ä¸ªæ ·æœ¬ç”Ÿæˆ 4 ä¸ªå€™é€‰æ–‡æœ¬
                candidate_texts = []
                for prompt in prompts:
                    for opt in options:
                        candidate_texts.append(prompt + " " + opt)

                # candidate_texts é•¿åº¦ä¸º batch_size * 4
                losses = compute_per_example_loss(model, tokenizer, candidate_texts)
                # losses å½¢çŠ¶: (batch_size * 4,)ï¼Œé‡å¡‘ä¸º (batch_size, 4)
                losses = losses.view(batch_size, 4)

                # æ¯ä¸ªæ ·æœ¬é€‰æ‹© loss æœ€ä½çš„é€‰é¡¹ä½œä¸ºé¢„æµ‹
                pred_indices = torch.argmin(losses, dim=1)  # shape: (batch_size,)
                pred_labels = [options[idx] for idx in pred_indices.cpu().numpy()]

                for pred, gold in zip(pred_labels, gold_labels):
                    if pred == gold:
                        correct += 1
                    total += 1
        accuracy = correct / total if total > 0 else 0
        return accuracy

    model.eval()
    accuracy = evaluate_accuracy_batch(model, tokenizer, dev_loader)
    return accuracy


#è®°å½•æ¯ä¸€è½®ç»“æŸæ—¶çš„dev_loss
def log_dev_loss_to_csv(epoch, lora_rank, dropout, lr, dev_loss, log_path):
    file_exists = Path(log_path).exists()
    with open(log_path, mode="a", newline='') as f:
        writer = csv.writer(f)
        if not file_exists:
            writer.writerow(["epoch", "lora_rank", "dropout", "lr", "dev_loss", "accuracy"])  # è¡¨å¤´
        writer.writerow([epoch, lora_rank, dropout, lr, f"{dev_loss:.4f}", ""])
#è®°å½•è®­ç»ƒç»“æŸåï¼Œåœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡
def log_final_accuracy_to_csv(lora_rank, dropout, lr, accuracy, log_path):
    file_exists = Path(log_path).exists()
    with open(log_path, mode="a", newline='') as f:
        writer = csv.writer(f)
        if not file_exists:
            writer.writerow(["epoch", "lora_rank", "dropout", "lr", "dev_loss", "accuracy"])
        writer.writerow(["final_accuracy", lora_rank, dropout, lr, "", f"{accuracy:.4f}"])

# âœ… Top 5 hyperparameter sets based on previous results
top_configs = [
    {"lora_rank": 16, "dropout": 0.1521, "lr": 0.000155},  # âœ… Trial 5
    {"lora_rank": 16, "dropout": 0.1840, "lr": 0.000080},  # âœ… Trial 13
    {"lora_rank": 16, "dropout": 0.1317, "lr": 0.000044},  # âœ… Trial 4
    {"lora_rank": 8,  "dropout": 0.2913, "lr": 0.000163},  # âœ… Trial 6
]



# âœ… Loop over top configs
for i, cfg in enumerate(top_configs):
    print(f"\nğŸš€ Running Trial {i} with lora_rank={cfg['lora_rank']}, dropout={cfg['dropout']}, lr={cfg['lr']:.6f}")
    score = train_model(
        lora_rank=cfg["lora_rank"],
        dropout=cfg["dropout"],
        learning_rate=cfg["lr"],
    )
    print(f"âœ… Trial {i}: params={{'lora_rank': {cfg['lora_rank']}, 'dropout': {cfg['dropout']}, 'lr': {cfg['lr']:.6f}}}, score={score:.4f}")
    save_path = "/home/ubuntu/meditron-medmcqa-finetune/data/log/train_13.csv"
    log_final_accuracy_to_csv(
        lora_rank=cfg["lora_rank"],
        dropout=cfg["dropout"],
        lr=cfg["lr"],
        accuracy=score,
        log_path=save_path
    )




