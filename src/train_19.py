import torch
from transformers import BitsAndBytesConfig
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.optim import AdamW
from torch.utils.data import DataLoader
from peft import LoraConfig, get_peft_model, TaskType
import os
from peft import PeftModel
from datasets import load_from_disk
from torch.utils.data._utils.collate import default_collate
import torch.nn.functional as F
import torch.nn as nn
import csv
from pathlib import Path
import random
import numpy as np


def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_seed(42)



def train_model(lora_rank=8, dropout=0.1, learning_rate=1e-4):

    # âœ… ç¯å¢ƒå˜é‡ä¼˜åŒ– CUDA æ˜¾å­˜
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64"
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

    # âœ… è·¯å¾„è®¾ç½®
    base_dir = Path("/home/ubuntu/meditron-medmcqa-finetune")
    model_path = base_dir / "models" / "meditron-7b"

    # âœ… åŠ è½½ Tokenizerï¼ˆæœ¬åœ°ï¼‰
    tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side="left", local_files_only=True)
    tokenizer.pad_token = tokenizer.eos_token

    # âœ… åŠ è½½ 8-bit æ¨¡å‹ï¼ˆæœ¬åœ°ï¼‰
    bnb_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=6.0, llm_int8_has_fp16_weight=False)
    model = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=bnb_config,
                                                 device_map={"": 0}, local_files_only=True)

    # âœ… åŠ è½½æ•°æ®
    processed_data = load_from_disk(base_dir / "data" / "processed_dataset_deepseek")
    train_dataset = processed_data["train"]
    dev_dataset = processed_data["dev"]

    def format_example(example):
        label_to_opt = {"A": "opa", "B": "opb", "C": "opc", "D": "opd"}
        # æ‹¼æ¥ prompt, label, å†’å·å’Œå¯¹åº”é€‰é¡¹å†…å®¹
        text = example["prompt"] + " " + example["label"] + ": " + example[label_to_opt[example["label"]]]
        return {"input_text": text}

    train_dataset = train_dataset.map(format_example)
    dev_dataset = dev_dataset.map(format_example)
    train_dataset = train_dataset.filter(lambda x: x is not None and "input_text" in x)
    dev_dataset = dev_dataset.filter(lambda x: x is not None and "input_text" in x)
    # åˆ’åˆ† train_eval_subsetï¼šä»è®­ç»ƒé›†åˆ’å‡º 1000 æ¡ç”¨äºè®­ç»ƒä¸­è¯„ä¼°å‡†ç¡®ç‡ï¼ˆearly stoppingï¼‰
    train_dataset = train_dataset.shuffle(seed=42)
    dev_dataset = dev_dataset.shuffle(seed=42)
    train_subset = train_dataset.select(range(10000))
    # æ‰“ä¹±éªŒè¯é›†ï¼Œåˆ’åˆ†å‡ºä¸¤ä¸ªéƒ¨åˆ†
    dev_eval_subset = dev_dataset.select(range(1000))  # â¬…ï¸ æ¯è½®è¯„ä¼°å‡†ç¡®ç‡
    dev_final_subset = dev_dataset.select(range(1000, len(dev_dataset)))  # â¬…ï¸ æœ€ç»ˆè¯„ä¼°å‡†ç¡®ç‡



    # âœ… æ„å»º Lora æ¨¡å‹
    lora_config = LoraConfig(r=lora_rank, lora_alpha=2 * lora_rank, lora_dropout=dropout, task_type=TaskType.CAUSAL_LM)
    model = get_peft_model(model, lora_config).to("cuda")

    # âœ… collate_fn
    def my_collate_fn(batch):
        for sample in batch:
            if sample.get("topic_name") is None:
                sample["topic_name"] = ""
            if sample.get("exp") is None:
                sample["exp"] = ""
        batch = [s for s in batch if s is not None]
        if not batch:
            raise ValueError("è¿‡æ»¤åæ²¡æœ‰æœ‰æ•ˆæ ·æœ¬")
        return torch.utils.data.dataloader.default_collate(batch)

    train_dataloader = DataLoader(train_subset, batch_size=3, shuffle=True, collate_fn=my_collate_fn)
    dev_eval_dataloader = DataLoader(dev_eval_subset, batch_size=3, shuffle=True, collate_fn=my_collate_fn)

    # âœ… å‡†ç¡®ç‡è¯„ä¼°å‡½æ•°
    def evaluate_model_accuracy(model, tokenizer, dev_dataset):
        def dev_collate_fn(batch):
            for sample in batch:
                sample["topic_name"] = sample.get("topic_name", "")
                sample["exp"] = sample.get("exp", "")

            return {
                "prompts": [s.get("prompt", "") for s in batch],
                "gold_labels": [s.get("label", "") for s in batch],
                "opa": [s.get("opa", "") for s in batch],
                "opb": [s.get("opb", "") for s in batch],
                "opc": [s.get("opc", "") for s in batch],
                "opd": [s.get("opd", "") for s in batch],
            }

        dev_loader = DataLoader(dev_dataset, batch_size=2, shuffle=False, collate_fn=dev_collate_fn)

        def compute_per_example_loss_after_answer(model, tokenizer, texts, answer_token_ids, max_length=768):
            """
            åœ¨éªŒè¯/æ¨æ–­é˜¶æ®µï¼Œå¯¹è¾“å…¥çš„å¤šæ¡æ–‡æœ¬ï¼Œåªè®¡ç®—ä»â€œAnswer:â€å¼€å§‹çš„tokençš„å¹³å‡lossï¼Œ
            å…¶å®ƒéƒ¨åˆ†ï¼ˆé—®å¥ã€é€‰é¡¹åˆ—è¡¨ç­‰ï¼‰è®¾ä¸º -100 ä¸çº³å…¥CEæŸå¤±ã€‚

            å‚æ•°:
              model: ä½ çš„LoRAå¾®è°ƒåçš„Causal LMæ¨¡å‹
              tokenizer: å¯¹åº”çš„åˆ†è¯å™¨ (ä¸ä¼šå˜åŠ¨)
              texts: list[str]ï¼Œé•¿åº¦ = batch_sizeï¼Œä¹Ÿå¯èƒ½æ˜¯4å€batch_size (æ¯ä¸ªæ ·æœ¬4é€‰é¡¹)
              answer_token_ids: tokenizer.encode("Answer:", add_special_tokens=False)
              max_length: åˆ†è¯é•¿åº¦ä¸Šé™

            è¿”å›:
              per_example_loss: Tensorï¼Œå½¢çŠ¶ [batch_size]ï¼Œè¡¨ç¤ºæ¯æ¡æ–‡æœ¬çš„å¹³å‡loss
            """
            # 1. åˆ†è¯
            inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=768)
            input_ids = inputs["input_ids"]
            attention_mask = inputs["attention_mask"]

            # âœ… 2. ç›´æ¥å¤ç”¨è®­ç»ƒæ—¶çš„ masking å‡½æ•°ï¼
            labels = mask_labels_before_answer(input_ids, tokenizer, answer_token_ids)

            # 4) æ”¾åˆ° GPU
            input_ids = input_ids.to(model.device)
            attention_mask = attention_mask.to(model.device)
            labels = labels.to(model.device)

            # 5) å‰å‘ä¼ æ’­ (ä¸ä½¿ç”¨ outputs.lossï¼Œæ‰‹åŠ¨æ‹¿ logits è®¡ç®—æ›´çµæ´»)
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            logits = outputs.logits  # shape: [B, L, vocab_size]

            # 6) åš shiftï¼šCausal LM é€šå¸¸è¦å¯¹ logits[:-1] å’Œ labels[1:]å¯¹é½
            shift_logits = logits[:, :-1, :].contiguous()
            shift_labels = labels[:, 1:].contiguous()  # shape: [B, L-1]

            # 7) è‡ªå®šä¹‰ token-level cross entropy
            loss_fct = nn.CrossEntropyLoss(reduction="none")
            loss_tokens = loss_fct(
                shift_logits.view(-1, shift_logits.size(-1)),
                shift_labels.view(-1)
            )
            # æŠŠå®ƒ reshape å› [B, L-1]
            loss_tokens = loss_tokens.view(shift_labels.size())

            # 8) åªå¯¹ label != -100 çš„ä½ç½®æ±‚å’Œï¼Œå†é™¤ä»¥æœ‰æ•ˆtokenæ•°
            valid_mask = (shift_labels != -100).float()
            per_example_loss = (loss_tokens * valid_mask).sum(dim=1) / (valid_mask.sum(dim=1) + 1e-8)

            return per_example_loss


        model.eval()
        total, correct = 0, 0
        with torch.no_grad():
            for batch in dev_loader:
                prompts = batch["prompts"]
                labels = batch["gold_labels"]
                opa = batch["opa"]
                opb = batch["opb"]
                opc = batch["opc"]
                opd = batch["opd"]
                all_options = ["A", "B", "C", "D"]
                option_texts = [opa, opb, opc, opd]  # æ¯ä¸ªéƒ½æ˜¯ list[str]ï¼Œé•¿åº¦ä¸º batch_size
                batch_size = len(prompts)
                candidate_texts = [] # æ„é€  candidate_textsï¼šæ¯ä¸ªæ ·æœ¬ 4 ä¸ªå¥å­ï¼Œæ€»å…± batch_size Ã— 4 ä¸ªå¥å­

                for i in range(batch_size):
                    for j, opt in enumerate(all_options):
                        option_content = option_texts[j][i]  # opa[i], opb[i], ...
                        full_text = f"{prompts[i]} {opt}: {option_content}"
                        candidate_texts.append(full_text)
                losses = compute_per_example_loss_after_answer(
                    model,
                    tokenizer,
                    candidate_texts,  # e.g.  batch_size * 4 ä¸ªå¥å­
                    answer_token_ids,
                    max_length=768
                )
                losses = losses.view(batch_size, 4) # reshape å› [batch_size, 4]
                preds = torch.argmin(losses, dim=1).cpu().numpy()
                pred_labels = [all_options[i] for i in preds]
                correct += sum(p == g for p, g in zip(pred_labels, labels))
                total += len(labels)
        return correct / total if total else 0

    def mask_labels_before_answer(input_ids: torch.Tensor, tokenizer, answer_tokens: list) -> torch.Tensor:
        """
        å¯¹ batch å†…çš„æ¯ä¸ªæ ·æœ¬ï¼Œåœ¨ input_ids ä¸­æ‰¾åˆ° `Answer:` çš„èµ·å§‹ä½ç½®ï¼Œ
        å°†è¯¥ä½ç½®ä¹‹å‰ï¼ˆå«â€œAnswer:â€æœ¬èº«ï¼‰æ‰€æœ‰ tokens çš„ label è®¾ä¸º -100ï¼Œä»¥ä¾¿åªå¯¹ç­”æ¡ˆä¸»ä½“éƒ¨åˆ†è®¡ç®—lossã€‚

        å‚æ•°:
          input_ids: [batch_size, seq_len] çš„å¼ é‡
          tokenizer: ä½ çš„åˆ†è¯å™¨å¯¹è±¡
          answer_tokens: å½¢å¦‚ tokenizer.encode("Answer:", add_special_tokens=False)

        è¿”å›:
          masked_labels: [batch_size, seq_len]ï¼Œåªæœ‰ â€œAnswer:â€ ä¹‹åéƒ¨åˆ†ä¸ºåŸ token idï¼Œå…¶ä½™è®¾ä¸º -100
        """
        batch_size, seq_len = input_ids.size()
        # å…ˆå¤åˆ¶ä¸€ä»½ input_ids ä½œä¸º labels
        masked_labels = input_ids.clone()

        # éå† batch ä¸­æ¯ä¸ªæ ·æœ¬
        for i in range(batch_size):
            row_ids = input_ids[i].tolist()
            start_idx = _find_answer_start_by_tokens(tokenizer, row_ids, answer_str="Answer:")

            if start_idx is not None:
                end_of_answer_prefix = start_idx + len(answer_tokens)
                # å°† end_of_answer_prefix ä¹‹å‰çš„å…¨éƒ¨ä½ç½®è®¾ä¸º -100
                masked_labels[i, :end_of_answer_prefix] = -100
            else:
                pass
            if start_idx is None:
                print("Answer token ids:", answer_token_ids)
                print("Example row:", tokenizer.convert_ids_to_tokens(input_ids[i].tolist()))
                print(f"[Warning] Sample {i} has no 'Answer:' token.")
        return masked_labels

    def _find_answer_start_by_tokens(tokenizer, input_ids, answer_str=" Answer:"):
        """
        ç›´æ¥é€šè¿‡ tokenizer åˆ†è¯ç»“æœä¸­çš„å­—ç¬¦ä¸²åŒ¹é…æ¥æ‰¾ "Answer:" èµ·å§‹ indexã€‚
        æ›´ç¨³ï¼Œä¸ä¾èµ– token ids å®Œå…¨ä¸€è‡´ã€‚
        """
        tokens = tokenizer.convert_ids_to_tokens(input_ids)
        answer_tokens = tokenizer.tokenize(answer_str)

        n, m = len(tokens), len(answer_tokens)
        for i in range(n - m + 1):
            if tokens[i:i + m] == answer_tokens:
                return i
        return None

    # def _find_answer_start(row_ids, answer_tokens):
    #     """
    #     åœ¨ row_ids è¿™æ¡åºåˆ—é‡Œï¼ˆå½¢å¦‚ [101, 234, 567, ...]ï¼‰ï¼Œ
    #     æ‰¾åˆ° answer_tokens å­åºåˆ—çš„ç¬¬ä¸€ä¸ªå‡ºç°ä½ç½®ã€‚å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¿”å› None
    #     """
    #     n = len(row_ids)
    #     m = len(answer_tokens)
    #     for start in range(n - m + 1):
    #         if row_ids[start:start + m] == answer_tokens:
    #             return start
    #     return None

    # âœ… Optimizer
    from torch.optim import AdamW
    optimizer = AdamW(model.parameters(), lr=learning_rate)

    # âœ… Training loop
    epochs = 2
    accumulation_steps = 5
    global_step = 0

    answer_token_ids = tokenizer.encode("Answer:", add_special_tokens=False)
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        for i, batch in enumerate(train_dataloader):
            inputs = tokenizer(batch["input_text"], return_tensors="pt", padding=True, truncation=True, max_length=896).to("cuda")
            input_ids = inputs["input_ids"].to(model.device)
            attention_mask = inputs["attention_mask"].to(model.device)
            # æ ¹æ®éœ€è¦å°† "Answer:" ä¹‹å‰çš„éƒ¨åˆ†maskæ‰
            labels = mask_labels_before_answer(input_ids, tokenizer, answer_token_ids).to(model.device)
            # å°†æ³¨æ„åŠ›æ©ç ä¹Ÿè¦å¸¦ä¸Š
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            (outputs.loss / accumulation_steps).backward()
            if (i + 1) % accumulation_steps == 0:
                optimizer.step()
                optimizer.zero_grad()
                global_step += 1
            if global_step % 300 == 0:
                torch.cuda.empty_cache()

        # âœ… epoch ç»“æŸæ—¶è¯„ä¼°
        model.eval()
        total_loss = 0
        with torch.no_grad():
            for dev_batch in dev_eval_dataloader:
                dev_inputs = tokenizer(dev_batch["input_text"], return_tensors="pt", padding=True, truncation=True, max_length=896).to("cuda")
                dev_outputs = model(**dev_inputs, labels=dev_inputs.input_ids)
                total_loss += dev_outputs.loss.item()
        avg_loss = total_loss / len(dev_eval_dataloader)

        save_path = base_dir / "data" / "log" / "train_19.csv"
        log_dev_loss_to_csv(epoch + 1, lora_rank, dropout, learning_rate, avg_loss, save_path)
        if epoch >= 0:
            accuracy = evaluate_model_accuracy(model, tokenizer, dev_eval_subset)
            print(f"Epoch {epoch + 1}, Dev Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}")
            log_final_accuracy_to_csv(lora_rank, dropout, learning_rate, accuracy, save_path,0)

    accuracy = evaluate_model_accuracy(model,tokenizer, dev_final_subset) #è®­ç»ƒå®Œæˆåï¼Œè¯„ä¼°æœ€ç»ˆçš„å‡†ç¡®ç‡
    save_path = base_dir / "data" / "log" / "train_19.csv"
    log_final_accuracy_to_csv(lora_rank, dropout, learning_rate, accuracy, save_path, 1)
    return accuracy


set_seed(42) #å›ºå®šéšæœºç§å­ï¼Œ

#è®°å½•æ¯ä¸€è½®ç»“æŸæ—¶çš„dev_loss
def log_dev_loss_to_csv(epoch, lora_rank, dropout, lr, dev_loss, log_path):
    file_exists = Path(log_path).exists()
    with open(log_path, mode="a", newline='') as f:
        writer = csv.writer(f)
        if not file_exists:
            writer.writerow(["epoch", "lora_rank", "dropout", "lr", "dev_loss", "accuracy"])  # è¡¨å¤´
        writer.writerow([epoch, lora_rank, dropout, lr, f"{dev_loss:.4f}", ""])
#è®°å½•è®­ç»ƒç»“æŸåï¼Œåœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡
def log_final_accuracy_to_csv(lora_rank, dropout, lr, accuracy, log_path, is_final=0):
    file_exists = Path(log_path).exists()
    with open(log_path, mode="a", newline='') as f:
        writer = csv.writer(f)
        if not file_exists:
            writer.writerow(["epoch", "lora_rank", "dropout", "lr", "dev_loss", "accuracy"])
        if not is_final:
            writer.writerow(["accuracy", lora_rank, dropout, lr, "", f"{accuracy:.4f}"])
        else:
            writer.writerow(["final_accuracy", lora_rank, dropout, lr, "", f"{accuracy:.4f}"])

# âœ… Top 5 hyperparameter sets based on previous results
top_configs = [
    {"lora_rank": 16, "dropout": 0.24, "lr": 0.00013}
]


# âœ… Loop over top configs
for i, cfg in enumerate(top_configs):
    print(f"\nğŸš€ Running Trial {i} with lora_rank={cfg['lora_rank']}, dropout={cfg['dropout']}, lr={cfg['lr']:.6f}")
    score = train_model(
        lora_rank=cfg["lora_rank"],
        dropout=cfg["dropout"],
        learning_rate=cfg["lr"],
    )
    print(f"âœ… Trial {i}: params={{'lora_rank': {cfg['lora_rank']}, 'dropout': {cfg['dropout']}, 'lr': {cfg['lr']:.6f}}}, score={score:.4f}")




