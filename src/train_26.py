import torch
from transformers import BitsAndBytesConfig
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.optim import AdamW
from torch.utils.data import DataLoader
from peft import LoraConfig, get_peft_model, TaskType
import os
from peft import PeftModel
from datasets import load_from_disk
from torch.utils.data._utils.collate import default_collate
import torch.nn.functional as F
import torch.nn as nn
import csv
from pathlib import Path
import random
import numpy as np
import wandb

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    # æ§åˆ¶ cuDNN å±‚é¢
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def train_model(lora_rank=8, dropout=0.1, learning_rate=1e-4, alpha = 0.5, seed = 42):

    set_seed(seed)
    # âœ… ç¯å¢ƒå˜é‡ä¼˜åŒ– CUDA æ˜¾å­˜
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64"
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

    class AttentionPooling(nn.Module):
        """
        å•å¤´ attention pooling:
        score = v^T tanh(W h_i)
        """

        def __init__(self, hidden_size: int, attn_hidden_size: int = 128, dropout: float = 0.1):
            super().__init__()
            self.W = nn.Linear(hidden_size, attn_hidden_size, bias=True)
            self.v = nn.Linear(attn_hidden_size, 1, bias=False)
            self.dropout = nn.Dropout(dropout)

        def forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor):
            hidden_states = hidden_states.to(self.W.weight.dtype)
            # hidden_states: (B, L, H); attention_mask: (B, L)
            scores = self.v(torch.tanh(self.W(hidden_states))).squeeze(-1)  # (B, L)

            # æŠŠ padding ä½ç½®è®¾ä¸º -infï¼Œé¿å…è¢«é€‰ä¸­
            scores = scores.masked_fill(attention_mask == 0, -1e4)

            attn_weights = F.softmax(scores, dim=-1)  # (B, L)
            attn_weights = self.dropout(attn_weights)  # å¯é€‰

            # (B, L, 1) * (B, L, H) â†’ (B, H)
            pooled = torch.bmm(attn_weights.unsqueeze(1), hidden_states).squeeze(1)
            return pooled  # (B, H)

    class DiscriminativeClassifier(nn.Module):
        def __init__(self, base_model: nn.Module, num_labels: int = 4,
                     attn_hidden_size: int = 128, attn_dropout: float = 0.15):
            super().__init__()
            self.base_model = base_model
            self.hidden_size = base_model.config.hidden_size
            self.pooler = AttentionPooling(self.hidden_size, attn_hidden_size, attn_dropout)
            self.classifier = nn.Linear(self.hidden_size, num_labels)

        def forward(
                self,
                input_ids,
                attention_mask=None,
                labels=None,
                gold_label=None,
                kl_alpha=0.5
        ):
            outputs = self.base_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_hidden_states=True,
                return_dict=True
            )
            last_hidden_state = outputs.hidden_states[-1]  # (B, L, H)
            pooled_output = self.pooler(last_hidden_state, attention_mask)
            logits = self.classifier(pooled_output)  # (B, 4)

            if labels is not None:
                log_probs = F.log_softmax(logits, dim=-1)
                kl_loss = F.kl_div(log_probs, labels, reduction='batchmean')
                ce_loss = F.cross_entropy(logits, gold_label)
                loss = kl_alpha * kl_loss + (1 - kl_alpha) * ce_loss
                return loss, logits
            else:
                return logits

    # âœ… è·¯å¾„è®¾ç½®
    base_dir = Path("/home/ubuntu/meditron-medmcqa-finetune")
    model_path = base_dir / "models" / "meditron-7b"

    # 1) åŠ è½½ Tokenizerï¼ˆæœ¬åœ°ï¼‰
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        padding_side="left",
        local_files_only=True
    )
    tokenizer.pad_token = tokenizer.eos_token
    # 2) é…ç½® BitsAndBytes (8-bit)
    bnb_config = BitsAndBytesConfig(
        load_in_8bit=True,
        llm_int8_threshold=6.0,
        llm_int8_has_fp16_weight=False
    )
    # 3) åŠ è½½åŸæ¨¡å‹ (CausalLM) åˆ° 8-bit
    base_causal_lm = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map="auto",
        quantization_config=bnb_config,  # 8-bit
        torch_dtype=torch.float16,
        local_files_only=True
    )
    # 4) ç»™åŸæ¨¡å‹æ³¨å…¥ LoRA adapter (è¿™é‡Œä¸€èˆ¬æ˜¯ Causal LM çš„ä»»åŠ¡ç±»å‹)
    lora_config = LoraConfig(
        r=lora_rank,
        lora_alpha=lora_rank*2,
        lora_dropout=dropout,
        task_type=TaskType.CAUSAL_LM
    )
    base_causal_lm = get_peft_model(base_causal_lm, lora_config)
    base_causal_lm.to("cuda")
    # 5) æ„å»ºåˆ¤åˆ«å¼åˆ†ç±»å™¨ (å¸¦LoRAçš„ CausalLM + åˆ†ç±»å¤´)
    model = DiscriminativeClassifier(base_model=base_causal_lm, num_labels=4)
    model.to("cuda")
    device = "cuda"

    # 6) ä½ æ˜¯å¦è¦å†»ç»“é™¤ LoRA æƒé‡ä»¥å¤–çš„å‚æ•°
    for name, param in model.base_model.named_parameters():
        if "lora" not in name.lower():
            param.requires_grad = False

    processed_data = load_from_disk(base_dir / "data" / "processed_dataset_soft_label")
    train_dataset = processed_data["train"]
    dev_dataset = processed_data["dev"]

    def format_example(example):
        label_map = {"A": 0, "B": 1, "C": 2, "D": 3}
        hard_label = label_map.get(example["label"], 0)

        option_prefix = ["A: ", "B: ", "C: ", "D: "]
        raw_options = [example["opa"], example["opb"], example["opc"], example["opd"]]
        options = [f"{prefix}{text}" for prefix, text in zip(option_prefix, raw_options)]

        return {
            "options": options,
            "hard_label": hard_label,
        }


    train_dataset = train_dataset.map(format_example)
    # åˆ’åˆ† train_eval_subsetï¼šä»è®­ç»ƒé›†åˆ’å‡º 1000 æ¡ç”¨äºè®­ç»ƒä¸­è¯„ä¼°å‡†ç¡®ç‡ï¼ˆearly stoppingï¼‰
    train_dataset = train_dataset.shuffle(seed=42)
    dev_dataset = dev_dataset.shuffle(seed=42)
    train_subset = train_dataset.select(range(10000))
    # æ‰“ä¹±éªŒè¯é›†ï¼Œåˆ’åˆ†å‡ºä¸¤ä¸ªéƒ¨åˆ†
    dev_eval_subset = dev_dataset.select(range(1000))  # â¬…ï¸ æ¯è½®è¯„ä¼°å‡†ç¡®ç‡
    dev_final_subset = dev_dataset.select(range(1000, len(dev_dataset)))  # â¬…ï¸ æœ€ç»ˆè¯„ä¼°å‡†ç¡®ç‡

    # âœ… collate_fn
    def my_collate_fn(batch):
        # batch: list[dict], æ¯ä¸ªdictåŒ…å« prompt/options/hard_label/soft_label
        new_batch = []
        for ex in batch:
            if ex.get("second_choice") is None:
                ex["second_choice"] = ""
            if ex is not None:
                new_batch.append(ex)
        if len(new_batch) == 0:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬")
        prompts = [ex["prompt"] for ex in new_batch]  # list of str
        hard_labels = [ex["hard_label"] for ex in new_batch]  # list of int
        soft_labels = [ex["soft_label"] for ex in new_batch]  # list of list of float

        # è½¬æˆtensor
        hard_labels = torch.tensor(hard_labels, dtype=torch.long)
        soft_labels = torch.tensor(soft_labels, dtype=torch.float32)

        return {
            "prompts": prompts,
            "hard_labels": hard_labels,
            "soft_labels": soft_labels
        }

    train_dataloader = DataLoader(train_subset, batch_size=3, shuffle=False, collate_fn=my_collate_fn)

    def evaluate_on_dev(
            model,
            tokenizer,
            dev_dataset,
            batch_size: int = 2,
            device: str = "cuda",
    ):
        """
        è¿”å›:
            acc      : float, éªŒè¯é›†æ•´ä½“å‡†ç¡®ç‡
            all_probs: Tensor (N,4), æ¯æ ·æœ¬å››é€‰é¡¹æ¦‚ç‡
            all_preds: Tensor (N,) , æ¯æ ·æœ¬é¢„æµ‹ç±»åˆ«ç´¢å¼• 0â€‘3
            all_gold : Tensor (N,) , æ¯æ ·æœ¬çœŸå®ç±»åˆ«ç´¢å¼•
        """

        # æŠŠé€‰é¡¹å­—æ¯æ˜ å°„æˆ 0â€’3
        choice2idx = {"A": 0, "B": 1, "C": 2, "D": 3}

        def dev_collate_fn(batch):
            batch = [ex for ex in batch if ex is not None]
            if len(batch) == 0:
                raise ValueError("æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬")
            # â¬‡ï¸ 1) prompts
            prompts = [ex["prompt"] for ex in batch]  # list[str]
            # â¬‡ï¸ 2) hard labelsï¼šæŠŠ "A"/"B"/â€¦ è½¬æˆ 0â€’3
            hard_labels = [choice2idx[ex["label"].strip()] for ex in batch]
            hard_labels = torch.tensor(hard_labels, dtype=torch.long)  # (B,)

            return {
                "prompts": prompts,  # list[str]
                "hard_labels": hard_labels,  # LongTensor (B,)
            }

        model.eval()
        dev_loader = DataLoader(
            dev_dataset,
            batch_size=batch_size,
            shuffle=False,
            collate_fn=dev_collate_fn,
        )

        all_probs, all_preds, all_gold = [], [], []
        total_loss = 0.0
        total_examples = 0

        with torch.no_grad():
            for batch in dev_loader:
                prompts = batch["prompts"]  # âœ”ï¸ ç”¨å¤æ•°é”®
                gold_labels = batch["hard_labels"].to(device)  # (B,)

                enc = tokenizer(
                    prompts,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=768,
                ).to(device)

                logits = model(
                    input_ids=enc["input_ids"],
                    attention_mask=enc["attention_mask"],
                )  # (B,4)

                probs = torch.softmax(logits, dim=-1)  # (B,4)
                preds = probs.argmax(dim=-1)  # (B,)

                # âœ… ç›´æ¥åœ¨è¿™é‡Œç®— cross_entropy loss
                loss = F.cross_entropy(logits, gold_labels)
                batch_size = gold_labels.size(0)
                total_loss += loss.item() * batch_size
                total_examples += batch_size

                all_probs.append(probs.cpu())
                all_preds.append(preds.cpu())
                all_gold.append(gold_labels.cpu())

        # æ‹¼æ¥æ•´ä¸ªéªŒè¯é›†
        all_probs = torch.cat(all_probs, dim=0)  # (N,4)
        all_preds = torch.cat(all_preds, dim=0)  # (N,)
        all_gold = torch.cat(all_gold, dim=0)  # (N,)
        # è®¡ç®—å‡†ç¡®ç‡
        acc = (all_preds == all_gold).float().mean().item()
        # è®¡ç®—æ•´ä¸ª dev é›†çš„å¹³å‡ loss
        avg_dev_loss = total_loss / total_examples

        model.train()
        return acc, all_probs, all_preds, all_gold, dev_loss

    # âœ… Optimizer
    from torch.optim import AdamW
    decay_params = []
    no_decay_params = []
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue
        if any(nd in name for nd in ["bias", "LayerNorm.weight"]):
            no_decay_params.append(param)
        else:
            decay_params.append(param)

    optimizer = AdamW(
        [
            {"params": decay_params, "weight_decay": 0.01},
            {"params": no_decay_params, "weight_decay": 0.0},
        ],
        lr=learning_rate
    )

    #å¼•å…¥ wanbï¼Œç”¨æ¥è®°å½•
    os.environ["WANDB_DIR"] = "/home/ubuntu/meditron-medmcqa-finetune/data"
    wandb.init(
        project="medmcqa-attnpooling",
        name=f"lr{learning_rate}_dropout{dropout}_alpha_{alpha}_seed{seed}",
        config={
            "learning_rate": learning_rate,
            "dropout": dropout,
            "kl_alpha": alpha,
            "seed": seed,
        },
        settings=wandb.Settings(code_dir=".")  # åªè·Ÿè¸ªä»£ç ï¼Œä¸è‡ªåŠ¨åŒæ­¥å¤§æ–‡ä»¶
    )
    # âœ… Training loop
    epochs = 3
    accumulation_steps = 5
    global_step = 0
    total_loss = 0.0

    for epoch in range(epochs):
        model.train()
        for i, batch in enumerate(train_dataloader):
            # --------- 1. æ•°æ®å‡†å¤‡ ---------
            prompts = batch["prompts"]  # list[str]
            # å¦‚æœ options å·²ç»åœ¨ prompt é‡Œï¼Œå°±ä¸ç”¨é¢å¤–å¤„ç†
            # tokenizer è‡ªåŠ¨æ‰¹é‡ç¼–ç å¹¶ Padding
            enc = tokenizer(
                prompts,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            input_ids = enc["input_ids"].to(device)
            attention_mask = enc["attention_mask"].to(device)

            # soft_labels: float32, shape (B,4)
            soft_labels = batch["soft_labels"].float().to(device)
            # hard_labels: int64  , shape (B,)
            hard_labels = batch["hard_labels"].long().to(device)

            # --------- 2. å‰å‘ + åå‘ ---------
            loss, logits = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=soft_labels,
                gold_label=hard_labels,  # å‘½åä¼ å‚é¿å…é¡ºåºé”™ä½
                kl_alpha=alpha
            )
            loss.backward()

            if (i + 1) % accumulation_steps == 0:
                optimizer.step()
                optimizer.zero_grad()
                global_step += 1
                total_loss += loss.item()

            wandb_save_step = 20
            if global_step % wandb_save_step == 0:
                avg_train_loss = total_loss / wandb_save_step
                wandb.log({"train_loss": avg_train_loss}, step=global_step)
                total_loss = 0.0

        if epoch >=0:
            save_path = base_dir / "data" / "log" / "train_26.csv"
            dev_acc, probs, preds, gold, dev_loss = evaluate_on_dev(
                model=model,
                tokenizer=tokenizer,
                dev_dataset=dev_eval_subset,
                batch_size=3,
                device="cuda",
            )
            print(f"Epoch: {epoch+1}, Dev accuracy: {dev_acc:.4f}")
            wandb.log({
                "dev_loss": dev_loss,
                "dev_acc": dev_acc
            }, step=epoch)
            log_final_accuracy_to_csv(epoch + 1, lora_rank, dropout, learning_rate, alpha, seed, dev_acc, save_path, 0)

    # save_path = base_dir / "data" / "log" / "train_26.csv"
    # dev_acc, probs, preds, gold, dev_loss = evaluate_on_dev(
    #     model=model,
    #     tokenizer=tokenizer,
    #     dev_dataset=dev_eval_subset,
    #     batch_size=3,
    #     device="cuda",
    # )
    # print(
    #     f"Final accuracy: {dev_acc:.4f} | "
    #     f"dropout={dropout:.3f}, "
    #     f"lr={learning_rate:.6f}, "
    #     f"alpha={alpha:.3f}, "
    #     f"seed={seed}"
    # )
    # log_final_accuracy_to_csv(epoch + 1, lora_rank, dropout, learning_rate, alpha, seed, dev_acc, save_path, 1)
    return dev_acc


def log_final_accuracy_to_csv(epoch, lora_rank, dropout, lr, alpha,seed, accuracy, log_path, is_final=0):
    file_exists = Path(log_path).exists()
    with open(log_path, mode="a", newline='') as f:
        writer = csv.writer(f)
        if not file_exists:
            writer.writerow(["epoch", "lora_rank", "dropout", "lr", "alpha", "seed", "accuracy"])
        if not is_final:
            writer.writerow([epoch, lora_rank, dropout, lr, alpha, seed, f"{accuracy:.4f}"])
        else:
            writer.writerow(["final_accuracy", lora_rank, dropout, lr, alpha, seed, f"{accuracy:.4f}"])


# top_configs = [
#     {"lora_rank": 16, "dropout": 0.15, "lr": 1e-4,  "alpha": 0.35},
#
# ]
# seed_list = [34, 7, 123]
#
# # 3ï¸âƒ£ é€è¶…å‚ç»„åˆ Ã— é€ seed è®­ç»ƒ â†’ å–å‡å€¼
# for i, cfg in enumerate(top_configs):
#     print(f"\nğŸš€ Hyperâ€‘Set {i} â†’ lora_rank={cfg['lora_rank']}, "
#           f"dropout={cfg['dropout']}, lr={cfg['lr']:.6f}, alpha={cfg['alpha']:.2f}")
#
#     seed_scores = []      # å­˜æ”¾åŒä¸€è¶…å‚ä¸‹ï¼Œä¸åŒ seed çš„éªŒè¯å‡†ç¡®ç‡
#
#     for sd in seed_list:
#         print(f"    â–¶ Seed {sd}...", end="", flush=True)
#
#         score = train_model(                 # <â€‘â€‘ ä½ çš„è®­ç»ƒå‡½æ•°
#             lora_rank      = cfg["lora_rank"],
#             dropout        = cfg["dropout"],
#             learning_rate  = cfg["lr"],
#             alpha          = cfg["alpha"],
#             seed           = sd             # å…³é”®ï¼šæŠŠ seed ä¼ è¿›å»
#         )
#
#         seed_scores.append(score)
#         print(f"  acc={score:.4f}")
#
#     # è®¡ç®—å¹³å‡ / æ–¹å·®
#     mean_acc = float(np.mean(seed_scores))
#     std_acc  = float(np.std(seed_scores))
#
#     print(f"âœ… Hyperâ€‘Set {i}  meanâ€‘acc={mean_acc:.4f}  std={std_acc:.4f}")


from pathlib import Path
import optuna
import numpy as np

# âœ… è®¾ç½®æ—¥å¿—ä¿å­˜ç›®å½•
log_dir = Path("/home/ubuntu/meditron-medmcqa-finetune/data/log")
log_dir.mkdir(parents=True, exist_ok=True)
db_path = log_dir / "train_26.db"

# âœ… å›ºå®š3ä¸ªç§å­
seed_list = [34, 7, 123]

# âœ… ç›®æ ‡å‡½æ•°ï¼šæ¯ç»„è¶…å‚è·‘3ä¸ªseedï¼Œå–å¹³å‡accä½œä¸ºç›®æ ‡
def objective(trial):
    # è¶…å‚æœç´¢ç©ºé—´
    lr = trial.suggest_float("learning_rate", 7e-5, 1.2e-4, log=True)
    alpha = trial.suggest_float("alpha", 0.25, 0.45)
    dropout = trial.suggest_float("dropout", 0.12, 0.2)

    acc_list = []

    # æ¯ä¸ª seed éƒ½ç‹¬ç«‹è®­ç»ƒä¸€é
    for sd in seed_list:
        score = train_model(
            lora_rank=16,
            dropout=dropout,
            learning_rate=lr,
            alpha=alpha,
            seed=sd   # ä¼ å…¥ä¸åŒseed
        )
        acc_list.append(score)

    mean_score = float(np.mean(acc_list))

    print(
        f"Trial {trial.number}: "
        f"params={{'lora_rank': {16}, 'dropout': {dropout:.3f}, 'lr': {lr:.6f}, 'alpha': {alpha:.3f}}}, "
        f"mean_acc={mean_score:.4f}"
    )

    return mean_score   # äº¤ç»™optunaçš„ä¼˜åŒ–å™¨å»maximize

# âœ… ä½¿ç”¨ SQLite æŒä¹…åŒ–
study = optuna.create_study(
    direction="maximize",
    study_name="meditron_lora_tuning",
    storage=f"sqlite:///{db_path}",
    load_if_exists=True
)

# âœ… å¼€å§‹æœç´¢
try:
    study.optimize(objective, n_trials=20, show_progress_bar=True)
except KeyboardInterrupt:
    print("ğŸ›‘ æ‰‹åŠ¨ä¸­æ–­è°ƒå‚ï¼Œå·²ä¿å­˜å½“å‰è¿›åº¦ã€‚")

# âœ… æœ€åè¾“å‡ºç»“æœ
print("ğŸ¯ æœ€ä¼˜å‚æ•°:", study.best_params)
print(f"âœ… æœ€ä¼˜å¹³å‡å‡†ç¡®ç‡: {study.best_value:.4f}")