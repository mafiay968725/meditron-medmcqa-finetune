import torch
from transformers import BitsAndBytesConfig
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.optim import AdamW
from torch.utils.data import DataLoader
from peft import LoraConfig, get_peft_model, TaskType
import os
from peft import PeftModel
from datasets import load_from_disk
from torch.utils.data._utils.collate import default_collate
import torch.nn.functional as F
import torch.nn as nn
import csv
from pathlib import Path
import random
import numpy as np


def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_seed(42)



def train_model(lora_rank=8, dropout=0.1, learning_rate=1e-4, alpha = 0.5):

    # âœ… ç¯å¢ƒå˜é‡ä¼˜åŒ– CUDA æ˜¾å­˜
    os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True,max_split_size_mb:64"

    # âœ… è·¯å¾„è®¾ç½®
    base_dir = Path("/home/ubuntu/meditron-medmcqa-finetune")
    model_path = base_dir / "models" / "meditron-7b"

    # âœ… åŠ è½½ Tokenizerï¼ˆæœ¬åœ°ï¼‰
    tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side="left", local_files_only=True)
    tokenizer.pad_token = tokenizer.eos_token

    # âœ… åŠ è½½ 8-bit æ¨¡å‹ï¼ˆæœ¬åœ°ï¼‰
    bnb_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=6.0, llm_int8_has_fp16_weight=False)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=bnb_config,
        device_map=None,  # â— ä¸æŒ‡å®šdevice mapï¼Œé¿å…åªç»‘ä¸€å¼ å¡
        local_files_only=True
    )



    # âœ… åŠ è½½æ•°æ®
    processed_data = load_from_disk(base_dir / "data" / "processed_dataset_soft_label")
    train_dataset = processed_data["train"]
    dev_dataset = processed_data["dev"]

    def format_example(example):
        label_map = {"A": 0, "B": 1, "C": 2, "D": 3}
        hard_label = label_map.get(example["label"], 0)

        option_prefix = ["A: ", "B: ", "C: ", "D: "]
        raw_options = [example["opa"], example["opb"], example["opc"], example["opd"]]
        options = [f"{prefix}{text}" for prefix, text in zip(option_prefix, raw_options)]


        return {
            "options": options,
            "hard_label": hard_label,
        }


    train_dataset = train_dataset.map(format_example)
    # åˆ’åˆ† train_eval_subsetï¼šä»è®­ç»ƒé›†åˆ’å‡º 1000 æ¡ç”¨äºè®­ç»ƒä¸­è¯„ä¼°å‡†ç¡®ç‡ï¼ˆearly stoppingï¼‰
    train_dataset = train_dataset.shuffle(seed=42)
    dev_dataset = dev_dataset.shuffle(seed=42)
    train_subset = train_dataset.select(range(10000))
    # æ‰“ä¹±éªŒè¯é›†ï¼Œåˆ’åˆ†å‡ºä¸¤ä¸ªéƒ¨åˆ†
    dev_eval_subset = dev_dataset.select(range(1000))  # â¬…ï¸ æ¯è½®è¯„ä¼°å‡†ç¡®ç‡
    dev_final_subset = dev_dataset.select(range(1000, len(dev_dataset)))  # â¬…ï¸ æœ€ç»ˆè¯„ä¼°å‡†ç¡®ç‡



    # âœ… æ„å»º Lora æ¨¡å‹
    lora_config = LoraConfig(r=lora_rank, lora_alpha=2 * lora_rank, lora_dropout=dropout, task_type=TaskType.CAUSAL_LM)
    model = get_peft_model(model, lora_config)
    model = nn.DataParallel(model)
    model = model.cuda()
    device = torch.device("cuda")

    # âœ… collate_fn
    def my_collate_fn(batch):
        # batch: list[dict], æ¯ä¸ªdictåŒ…å« prompt/options/hard_label/soft_label
        new_batch = []
        for ex in batch:
            if ex.get("second_choice") is None:
                ex["second_choice"] = ""
            if ex is not None:
                new_batch.append(ex)
        if len(new_batch) == 0:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬")
        prompts = [ex["prompt"] for ex in new_batch]          # list of str
        options_list = [ex["options"] for ex in new_batch]    # list of list of str
        hard_labels = [ex["hard_label"] for ex in new_batch]  # list of int
        soft_labels = [ex["soft_label"] for ex in new_batch]  # list of list of float

        # è½¬æˆtensor
        hard_labels = torch.tensor(hard_labels, dtype=torch.long)
        soft_labels = torch.tensor(soft_labels, dtype=torch.float32)

        return {
            "prompts": prompts,
            "options": options_list,
            "hard_labels": hard_labels,
            "soft_labels": soft_labels
        }
    train_dataloader = DataLoader(train_subset, batch_size=4, shuffle=True, collate_fn=my_collate_fn)

    # âœ… å‡†ç¡®ç‡è¯„ä¼°å‡½æ•°
    def evaluate_model_accuracy(model, tokenizer, dev_dataset):
        def dev_collate_fn(batch):

            return {
                "prompts": [s.get("prompt", "") for s in batch],
                "gold_labels": [s.get("label", "") for s in batch],
                "opa": [s.get("opa", "") for s in batch],
                "opb": [s.get("opb", "") for s in batch],
                "opc": [s.get("opc", "") for s in batch],
                "opd": [s.get("opd", "") for s in batch],
            }

        dev_loader = DataLoader(dev_dataset, batch_size=4, shuffle=False, collate_fn=dev_collate_fn)

        def compute_per_example_loss_after_answer(model, tokenizer, texts,max_length=768):
            # 1. åˆ†è¯
            inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=768)
            input_ids = inputs["input_ids"]
            attention_mask = inputs["attention_mask"]

            # âœ… 2. ç›´æ¥å¤ç”¨è®­ç»ƒæ—¶çš„ masking å‡½æ•°ï¼
            labels = mask_labels_before_answer(input_ids, tokenizer)

            # 4) æ”¾åˆ° GPU
            input_ids = input_ids.to(device)
            attention_mask = attention_mask.to(device)
            labels = labels.to(device)

            # 5) å‰å‘ä¼ æ’­ (ä¸ä½¿ç”¨ outputs.lossï¼Œæ‰‹åŠ¨æ‹¿ logits è®¡ç®—æ›´çµæ´»)
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            logits = outputs.logits  # shape: [B, L, vocab_size]

            # 6) åš shiftï¼šCausal LM é€šå¸¸è¦å¯¹ logits[:-1] å’Œ labels[1:]å¯¹é½
            shift_logits = logits[:, :-1, :].contiguous()
            shift_labels = labels[:, 1:].contiguous()  # shape: [B, L-1]

            # 7) è‡ªå®šä¹‰ token-level cross entropy
            loss_fct = nn.CrossEntropyLoss(reduction="none")
            loss_tokens = loss_fct(
                shift_logits.view(-1, shift_logits.size(-1)),
                shift_labels.view(-1)
            )
            # æŠŠå®ƒ reshape å› [B, L-1]
            loss_tokens = loss_tokens.view(shift_labels.size())

            # 8) åªå¯¹ label != -100 çš„ä½ç½®æ±‚å’Œï¼Œå†é™¤ä»¥æœ‰æ•ˆtokenæ•°
            valid_mask = (shift_labels != -100).float()
            per_example_loss = (loss_tokens * valid_mask).sum(dim=1) / (valid_mask.sum(dim=1) + 1e-8)

            return per_example_loss

        torch.cuda.empty_cache()
        model.eval()
        device = torch.device("cuda")
        total, correct = 0, 0
        with torch.no_grad():
            for batch in dev_loader:
                prompts = batch["prompts"]
                labels = batch["gold_labels"]
                opa = batch["opa"]
                opb = batch["opb"]
                opc = batch["opc"]
                opd = batch["opd"]
                all_options = ["A", "B", "C", "D"]
                option_texts = [opa, opb, opc, opd]  # æ¯ä¸ªéƒ½æ˜¯ list[str]ï¼Œé•¿åº¦ä¸º batch_size
                batch_size = len(prompts)
                candidate_texts = [] # æ„é€  candidate_textsï¼šæ¯ä¸ªæ ·æœ¬ 4 ä¸ªå¥å­ï¼Œæ€»å…± batch_size Ã— 4 ä¸ªå¥å­

                for i in range(batch_size):
                    for j, opt in enumerate(all_options):
                        option_content = option_texts[j][i]  # opa[i], opb[i], ...
                        full_text = f"{prompts[i]} {opt}: {option_content}"
                        candidate_texts.append(full_text)
                losses = compute_per_example_loss_after_answer(
                    model,
                    tokenizer,
                    candidate_texts,  # e.g.  batch_size * 4 ä¸ªå¥å­
                    max_length=768
                )
                losses = losses.view(batch_size, 4) # reshape å› [batch_size, 4]
                preds = torch.argmin(losses, dim=1).cpu().numpy()
                pred_labels = [all_options[i] for i in preds]
                correct += sum(p == g for p, g in zip(pred_labels, labels))
                total += len(labels)
        return correct / total if total else 0

    def mask_labels_before_answer(input_ids: torch.Tensor, tokenizer) -> torch.Tensor:
        batch_size, seq_len = input_ids.size()
        # å…ˆå¤åˆ¶ä¸€ä»½ input_ids ä½œä¸º labels
        masked_labels = input_ids.clone()

        # éå† batch ä¸­æ¯ä¸ªæ ·æœ¬
        for i in range(batch_size):
            row_ids = input_ids[i].tolist()
            start_idx = _find_answer_pair_by_tokens(tokenizer, row_ids)
            if start_idx is not None:
                end_of_answer_prefix = start_idx + 2  # å› ä¸ºæ˜¯ Answer + : ä¸¤ä¸ª token
                masked_labels[i, :end_of_answer_prefix] = -100 #Answer: å‰çš„å…¨éƒ¨mask
            else:
                pass
            if start_idx is None:
                print(f"[Warning] Sample {i} has no 'Answer:' token.")

        return masked_labels


    def _find_answer_pair_by_tokens(tokenizer, input_ids):
        tokens = tokenizer.convert_ids_to_tokens(input_ids)
        target_seq = ["Answer", ":"]
        n, m = len(tokens), len(target_seq)
        for i in range(n - m + 1):
            if tokens[i:i + m] == target_seq:
                return i
        return None

    def compute_ce_kl_loss(
            model,
            input_ids: torch.Tensor,  # [batch_size*4, seq_len]
            attention_mask: torch.Tensor,  # [batch_size*4, seq_len]
            labels: torch.Tensor,  # [batch_size*4, seq_len]  (å« -100)
            hard_labels: torch.Tensor,  # [batch_size], 0~3
            soft_labels: torch.Tensor,  # [batch_size, 4], sum=1
            alpha: float = 0.5
    ):
        """
        å°†â€œPrompt + è§£é‡Š + é€‰é¡¹â€å…±4æ¡åºåˆ—çš„ token-level loss è®¡ç®—å¹¶æ±‡æ€»ï¼Œ
        å¾—åˆ°å„é€‰é¡¹çš„å¹³å‡ NLL -> å†åš (1âˆ’alpha)*CE + alpha*KLã€‚

        å‚æ•°ï¼š
        - modelï¼šè‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼ˆGPT/LLaMAç­‰ï¼‰
        - input_ids / attention_maskï¼šå¯¹ 4*batch_size æ¡åºåˆ—åšäº†æ‹¼æ¥
        - labelsï¼šmaskæ‰ä¸éœ€è¦è®­ç»ƒçš„ tokenï¼ˆ-100ï¼‰ï¼Œåªä¿ç•™ç­”æ¡ˆéƒ¨åˆ†ï¼›å½¢çŠ¶åŒ input_ids
        - hard_labelsï¼šæ¯æ¡æ ·æœ¬çš„æ­£ç¡®é€‰é¡¹ä¸‹æ ‡ [batch_size]ï¼Œå¦‚ 0/1/2/3
        - soft_labelsï¼šteacher soft labelåˆ†å¸ƒ [batch_size,4]
        - alphaï¼šKLæƒé‡

        è¿”å›ï¼š
        - lossï¼šæ ‡é‡ï¼Œç”¨äº loss.backward()
        """

        # 1) å‰å‘ä¼ æ’­ï¼ˆä¸ä½¿ç”¨ model(..., labels=...)ï¼Œé¿å…é»˜è®¤reduction=meanï¼‰
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits  # shape: [batch_size*4, seq_len, vocab_size]

        # 2) å¯¹é½æ ‡ç­¾ï¼ˆshiftï¼‰ï¼Œè®¡ç®—æ¯ä¸ªä½ç½® token-level CE (reduction='none')
        #    æ³¨æ„åªå¯¹ seq_len-1 ä½ç½®åšé¢„æµ‹ï¼ˆè‡ªå›å½’ï¼‰
        shift_logits = logits[:, :-1, :].contiguous()  # [batch_size*4, seq_len-1, vocab_size]
        shift_labels = labels[:, 1:].contiguous()  # [batch_size*4, seq_len-1]

        loss_fct = nn.CrossEntropyLoss(reduction='none')
        token_level_loss = loss_fct(
            shift_logits.view(-1, shift_logits.size(-1)),
            shift_labels.view(-1)
        )
        # => shape [ (batch_size*4)*(seq_len-1) ]

        token_level_loss = token_level_loss.view(shift_labels.size())  # [batch_size*4, seq_len-1]

        # 3) å»é™¤ -100 æ ‡è®°ï¼ˆä¸è®¡å…¥lossçš„promptéƒ¨åˆ†ï¼‰
        valid_mask = (shift_labels != -100).float()
        sum_loss_per_seq = (token_level_loss * valid_mask).sum(dim=1)  # [batch_size*4]
        token_count = valid_mask.sum(dim=1)  # [batch_size*4]
        avg_nll_per_seq = sum_loss_per_seq / (token_count + 1e-8)  # [batch_size*4]

        # 4) reshapeæˆ [batch_size, 4]
        batch_size = hard_labels.size(0)
        avg_nll_per_seq = avg_nll_per_seq.view(batch_size, 4)  # [batch_size,4]

        # 5) æŠŠè´Ÿå¯¹æ•°ä¼¼ç„¶å–è´Ÿå€¼ => ä½œä¸º 4ä¸ªé€‰é¡¹çš„"åˆ†æ•°"
        #    score_for_each_optionæ›´å¤§ => è¯´æ˜é€‰é¡¹æ›´å¯èƒ½
        score_for_each_option = - avg_nll_per_seq  # [batch_size,4]

        # 6) è®¡ç®—KLï¼šKL(P_teacher || P_student)
        #    å…ˆå¯¹score_for_each_optionåš log_softmax => log P_student
        log_student_probs = F.log_softmax(score_for_each_option, dim=-1)  # [batch_size,4]

        # teacher è½¯æ ‡ç­¾ [batch_size,4], sum=1
        kl_loss_fct = nn.KLDivLoss(reduction='batchmean')
        kl_loss = kl_loss_fct(log_student_probs, soft_labels)

        # 7) è®¡ç®—CEï¼šæŠŠscore_for_each_optionå½“åš4ç±»åˆ†ç±»logits
        ce_loss_fct = nn.CrossEntropyLoss()
        ce_loss = ce_loss_fct(score_for_each_option, hard_labels)  # shape []

        # 8) æ•´åˆloss
        loss = (1 - alpha) * ce_loss + alpha * kl_loss

        return loss

    # âœ… Optimizer
    from torch.optim import AdamW
    optimizer = AdamW(model.parameters(), lr=learning_rate)

    # âœ… Training loop
    epochs = 2
    accumulation_steps = 4
    global_step = 0

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        for i, batch in enumerate(train_dataloader):
            prompts = batch["prompts"]  # list[str]
            options_list = batch["options"]  # list[list[str]]
            hard_labels = batch["hard_labels"].to(device)# [batch_size]
            soft_labels = batch["soft_labels"].to(device)  # [batch_size,4]

            # æŠŠ batch_size æ¡æ•°æ®æ¯æ¡4ä¸ªé€‰é¡¹ â†’ æ‹¼æˆ batch_size*4 ä¸ªæ–‡æœ¬
            input_texts = []
            for idx, prompt_text in enumerate(prompts):
                for option_text in options_list[idx]:
                    # e.g. "Prompt + è§£é‡Š +  'Answer:' + é€‰é¡¹"
                    # æ ¹æ®ä½ å®é™…é€»è¾‘æ‹¼èµ·æ¥å³å¯
                    text = f"{prompt_text}{option_text}"
                    input_texts.append(text)

            # tokenizer
            inputs = tokenizer(
                input_texts,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=896
            )

            input_ids = inputs["input_ids"].to(device)  # [batch_size*4, seq_len]
            attention_mask = inputs["attention_mask"].to(device)

            # åˆ©ç”¨ä½ å†™å¥½çš„å‡½æ•°ï¼ŒæŠŠ prompt/è§£é‡Šéƒ¨åˆ†æ ‡ç­¾è®¾æˆ -100ï¼Œåªä¿ç•™ "Answer" ä¹‹ç±»éœ€è¦è®¡ç®—lossçš„åœ°æ–¹
            labels = mask_labels_before_answer(input_ids, tokenizer).to(device)

            # è®¡ç®— (1-alpha)*CE + alpha*KL
            loss = compute_ce_kl_loss(
                model=model,
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels,
                hard_labels=hard_labels,
                soft_labels=soft_labels,
                alpha=alpha  # or any
            )

            # æ¢¯åº¦ç´¯ç§¯
            (loss / accumulation_steps).backward()

            if (i + 1) % accumulation_steps == 0:
                optimizer.step()
                optimizer.zero_grad()

        # âœ… epoch ç»“æŸæ—¶è¯„ä¼°
        # model.eval()
        # save_path = base_dir / "data" / "log" / "train_22.csv"
        # if epoch >= 0:
        #     accuracy = evaluate_model_accuracy(model, tokenizer, dev_eval_subset)
        #     print(f"Epoch {epoch + 1},  Accuracy: {accuracy:.4f}")
        #     log_final_accuracy_to_csv(epoch+1, lora_rank, dropout, learning_rate, accuracy, save_path,0)

    accuracy = evaluate_model_accuracy(model,tokenizer, dev_eval_subset) #è®­ç»ƒå®Œæˆåï¼Œè¯„ä¼°æœ€ç»ˆçš„å‡†ç¡®ç‡
    save_path = base_dir / "data" / "log" / "train_22.csv"
    log_final_accuracy_to_csv(epochs, lora_rank, dropout, learning_rate, accuracy, save_path, 1)
    return accuracy


set_seed(42) #å›ºå®šéšæœºç§å­ï¼Œ
print("Using", torch.cuda.device_count(), "GPUs") #æ‰“å°å¯ç”¨GPUæ•°é‡

#è®°å½•æ¯ä¸€è½®ç»“æŸæ—¶çš„dev_loss
def log_dev_loss_to_csv(epoch, lora_rank, dropout, lr, dev_loss, log_path):
    file_exists = Path(log_path).exists()
    with open(log_path, mode="a", newline='') as f:
        writer = csv.writer(f)
        if not file_exists:
            writer.writerow(["epoch", "lora_rank", "dropout", "lr", "dev_loss", "accuracy"])  # è¡¨å¤´
        writer.writerow([epoch, lora_rank, dropout, lr, f"{dev_loss:.4f}", ""])
#è®°å½•è®­ç»ƒç»“æŸåï¼Œåœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡
def log_final_accuracy_to_csv(epoch, lora_rank, dropout, lr, accuracy, log_path, is_final=0):
    file_exists = Path(log_path).exists()
    with open(log_path, mode="a", newline='') as f:
        writer = csv.writer(f)
        if not file_exists:
            writer.writerow(["epoch", "lora_rank", "dropout", "lr", "dev_loss", "accuracy"])
        if not is_final:
            writer.writerow([epoch, lora_rank, dropout, lr, "", f"{accuracy:.4f}"])
        else:
            writer.writerow(["final_accuracy", lora_rank, dropout, lr, "", f"{accuracy:.4f}"])


import optuna
import joblib
from pathlib import Path

# âœ… è®¾ç½®ä¿å­˜è·¯å¾„
log_dir = Path("/home/ubuntu/meditron-medmcqa-finetune/data/log")
log_dir.mkdir(parents=True, exist_ok=True)  # å¦‚æœä¸å­˜åœ¨å°±åˆ›å»º

# âœ… è®¾å®šæ•°æ®åº“å’Œæ–‡ä»¶å
db_path = log_dir / "train_22.db"


def objective(trial):
    lr = trial.suggest_float("learning_rate", 2e-5, 1.2e-4, log=True)
    alpha = trial.suggest_float("alpha", 0.2, 0.8)
    score = train_model(
        lora_rank=16,
        dropout=0.15,
        learning_rate=lr,
        alpha = alpha
    )

    print(
        f"Trial {trial.number}: params={{'lora_rank': {16}, 'dropout': {0.15}, 'lr': {lr:.6f}, 'alpha': {alpha:.2f}}}, score={score:.4f}")
    return score

# âœ… ä½¿ç”¨ SQLite å­˜å‚¨ï¼Œä¿å­˜è‡³æŒ‡å®šè·¯å¾„
study = optuna.create_study(
    direction="maximize",
    study_name="meditron_lora_tuning",
    storage=f"sqlite:///{db_path}",
    load_if_exists=True
)

try:
    study.optimize(objective, n_trials=10, show_progress_bar=True)
except KeyboardInterrupt:
    print("ğŸ›‘ æ‰‹åŠ¨ä¸­æ–­è°ƒå‚ï¼Œå·²ä¿å­˜å½“å‰è¿›åº¦ã€‚")

# âœ… è¾“å‡ºå¹¶ä¿å­˜
print("ğŸ¯ æœ€ä¼˜å‚æ•°:", study.best_params)
print(f"âœ… æœ€ä¼˜å‡†ç¡®ç‡: {study.best_value:.4f}")