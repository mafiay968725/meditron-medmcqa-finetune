import torch
from transformers import BitsAndBytesConfig
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.optim import AdamW
from torch.utils.data import DataLoader
from peft import LoraConfig, get_peft_model, TaskType
import sys
import os


os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True" #å¯ç”¨ PyTorch çš„æ›´æ™ºèƒ½æ˜¾å­˜åˆ†é…ç­–ç•¥
model_name_or_path =  "/root/meditron-medmcqa-finetune/models/meditron-7b" # æˆ–è€…ä½ æœ¬åœ°è·¯å¾„

# 1) åŠ è½½ Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side="left")
# GPT/LLAMA ç³»æ¨¡å‹é€šå¸¸ç”¨å·¦ä¾§ padding
tokenizer.pad_token = tokenizer.eos_token  # é¿å…å‡ºç°è­¦å‘Š

# 2) é…ç½® 8-bit é‡åŒ–
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,          # 8bit é‡åŒ–
    llm_int8_threshold=6.0,     # ä¸€äº›é»˜è®¤é˜ˆå€¼
    llm_int8_has_fp16_weight=False,
)

# 3) åŠ è½½æ¨¡å‹ (8-bit)
model = AutoModelForCausalLM.from_pretrained(
    model_name_or_path,
    quantization_config=bnb_config,
    device_map="auto"  # æ ¹æ®æ˜¾å­˜è‡ªåŠ¨åˆ†é…åˆ° GPU
)


from datasets import load_from_disk

processed_data = load_from_disk("/root/meditron-medmcqa-finetune/data/processed_dataset")
# é‡Œé¢åŒ…å« train/dev/test åˆ†å‰²
train_dataset = processed_data["train"]
dev_dataset = processed_data["dev"]

def format_example(example):
    # å¦‚æœä½ ä¹‹å‰åœ¨ "prompt" å­—æ®µå·²ç»åŒ…å«äº† "Answer: ???"
    # å¹¶ä¸” "label" æ˜¯ "A/B/C/D"
    # è¿™é‡Œç›´æ¥æŠŠå®ƒæ‹¼åˆ° prompt åé¢å³å¯
    text = example["prompt"] + " " + example["label"]  # ä¾‹å¦‚: "...Answer: C"
    return {"input_text": text}





# 3) è®¾ç½®Loraé…ç½®
lora_config = LoraConfig(
    r=8,  # Rank of the adapter
    lora_alpha=16,  # Lora scaling factor
    lora_dropout=0.1,  # Dropout
    task_type=TaskType.CAUSAL_LM,  # Causal language modeling
)

# 4) æ„å»ºLoraæ¨¡å‹
model = get_peft_model(model, lora_config)
model.to("cuda")  # ç¡®ä¿æ¨¡å‹åœ¨GPUä¸Š
model.print_trainable_parameters()  # çœ‹å¯è®­ç»ƒå‚æ•°æ•°é‡

# 5) æ•°æ®åŠ è½½å™¨
train_dataset = train_dataset.map(format_example)
dev_dataset = dev_dataset.map(format_example)
train_dataset = train_dataset.filter(lambda x: x is not None and "input_text" in x)
dev_dataset = dev_dataset.filter(lambda x: x is not None and "input_text" in x)
train_subset = train_dataset.shuffle(seed=42).select(range(30000)) #æ„å»ºä¸€ä¸ª10kçš„å­è®­ç»ƒé›†ï¼Œè¿›è¡Œè¯•éªŒ
dev_subset = dev_dataset.shuffle(seed=42).select(range(1000)) #å…ˆç”¨éªŒè¯é›†çš„ä¸€éƒ¨åˆ†è¿›è¡Œè®¡ç®—

from torch.utils.data._utils.collate import default_collate
def my_collate_fn(batch):
    for sample in batch:
        if sample is not None:
            # å°† None æ›¿æ¢ä¸ºé»˜è®¤å€¼ï¼Œä¾‹å¦‚ç©ºå­—ç¬¦ä¸²
            if sample.get("topic_name") is None:
                sample["topic_name"] = ""
            if sample.get("exp") is None:
                sample["exp"] = ""
    # è¿‡æ»¤æ‰æ•´ä½“ä¸º None çš„æ ·æœ¬
    filtered_batch = [sample for sample in batch if sample is not None]
    if len(filtered_batch) == 0:
        raise ValueError("è¿‡æ»¤åï¼Œå½“å‰æ‰¹æ¬¡æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬ï¼Œè¯·æ£€æŸ¥æ•°æ®é¢„å¤„ç†é€»è¾‘")

    return torch.utils.data.dataloader.default_collate(filtered_batch)
# ç„¶ååœ¨ DataLoader ä¸­ä½¿ç”¨ï¼š
train_dataloader = DataLoader(train_subset, batch_size=7, shuffle=True, collate_fn=my_collate_fn)
dev_dataloader = DataLoader(dev_subset, batch_size=7, collate_fn=my_collate_fn)


# 6) ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
optimizer = AdamW(model.parameters(), lr=7e-5)


# 7) è®­ç»ƒå¾ªç¯
eval_interval = 900  # æ¯300æ¬¡ä¼˜åŒ–åè¯„ä¼°ä¸€æ¬¡
epochs = 5
best_dev_loss = float("inf")  # ç”¨æ¥ä¿å­˜å½“å‰æœ€å°çš„éªŒè¯é›†æŸå¤±

accumulation_steps = 2  # æ¯2ä¸ªmini-batchç´¯ç§¯ä¸€æ¬¡æ¢¯åº¦ => æœ‰æ•ˆbatch_size=8Ã—2=16
global_step = 0         # è®°å½•çœŸå®çš„ä¼˜åŒ–æ­¥æ•°ï¼ˆæ¯å®Œæˆä¸€æ¬¡optimizer.step()å°±+1ï¼‰

for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()  # ç¡®ä¿æ¯ä¸ªepochå¼€å§‹æ—¶æ¢¯åº¦ä¸º0

    for i, batch in enumerate(train_dataloader):
        # 1. å‡†å¤‡è¾“å…¥
        if "input_text" in batch:
            inputs = tokenizer(batch["input_text"], return_tensors="pt",
                               padding=True, truncation=True, max_length=1024).to("cuda")
        else:
            print("âŒ ç¼ºå¤± input_text çš„æ ·æœ¬ï¼š", batch)
            sys.exit("â›” ç¨‹åºå·²ç»ˆæ­¢ï¼Œå› ä¸ºæœ‰æ ·æœ¬ç¼ºå¤± input_text")

        # 2. å‰å‘ä¼ æ’­
        labels = inputs.input_ids
        outputs = model(**inputs, labels=labels)
        loss = outputs.loss

        # 3. æ¢¯åº¦ç´¯ç§¯: æ¯æ¬¡åªåä¼  loss / accumulation_steps
        (loss / accumulation_steps).backward()

        # 4. ç´¯è®¡åˆ°ä¸€å®šæ­¥æ•°ï¼Œå†æ‰§è¡Œä¸€æ¬¡ä¼˜åŒ–
        if (i + 1) % accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()
            global_step += 1

            # 5. æ¯ eval_interval ä¸ª "ä¼˜åŒ–æ­¥" è¿›è¡Œä¸€æ¬¡è¯„ä¼°
            if global_step % 300 == 0:
                torch.cuda.empty_cache()
            if global_step % eval_interval == 0:
                model.eval()
                total_loss = 0
                with torch.no_grad():
                    for dev_batch in dev_dataloader:
                        dev_inputs = tokenizer(dev_batch["input_text"],
                                               return_tensors="pt", padding=True, truncation=True, max_length=1024).to("cuda")
                        dev_labels = dev_inputs.input_ids
                        dev_outputs = model(**dev_inputs, labels=dev_labels)
                        total_loss += dev_outputs.loss.item()
                avg_loss = total_loss / len(dev_dataloader)
                print(f"Epoch {epoch + 1}, Global Step {global_step}, Dev Loss: {avg_loss:.4f}")

                # ä¿å­˜æœ€ä¼˜æ¨¡å‹
                if avg_loss < best_dev_loss:
                    best_dev_loss = avg_loss
                    model.save_pretrained("/root/meditron-medmcqa-finetune/data/train_8/best")
                    print(f"ğŸ’¾ æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜ï¼Œå½“å‰ Dev Loss: {avg_loss:.4f}")
                model.train()

    # æ¯ä¸ª epoch ç»“æŸåä¿å­˜ä¸€æ¬¡æ¨¡å‹
    save_path = f"/root/meditron-medmcqa-finetune/data/train_8/epoch_{epoch + 1}"
    model.save_pretrained(save_path)
    if epoch == 0:
        tokenizer.save_pretrained("/root/meditron-medmcqa-finetune/data/train_8/tokenizer")
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³ {save_path}")