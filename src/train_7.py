import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.optim import AdamW
from torch.utils.data import DataLoader
from peft import LoraConfig, get_peft_model, TaskType
import sys
import os
from torch.cuda.amp import autocast, GradScaler

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"  # å¯ç”¨ PyTorch çš„æ›´æ™ºèƒ½æ˜¾å­˜åˆ†é…ç­–ç•¥
model_name_or_path = "/root/meditron-medmcqa-finetune/models/meditron-7b"  # æˆ–è€…ä½ æœ¬åœ°è·¯å¾„

# 1) åŠ è½½ Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side="left")
tokenizer.pad_token = tokenizer.eos_token  # é¿å…å‡ºç°è­¦å‘Š

# 2) åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨ FP16 æ··åˆç²¾åº¦ï¼ˆæ›¿æ¢åŸæ¥çš„ 8-bit é‡åŒ–ï¼‰
model = AutoModelForCausalLM.from_pretrained(
    model_name_or_path,
    torch_dtype=torch.float16,  # ä½¿ç”¨ FP16 åŠ è½½ï¼Œé™ä½æ˜¾å­˜å ç”¨
    device_map="auto"  # æ ¹æ®æ˜¾å­˜è‡ªåŠ¨åˆ†é…åˆ° GPU
)

from datasets import load_from_disk

processed_data = load_from_disk("/root/meditron-medmcqa-finetune/data/processed_dataset")
# é‡Œé¢åŒ…å« train/dev/test åˆ†å‰²
train_dataset = processed_data["train"]
dev_dataset = processed_data["dev"]


def format_example(example):
    """
    å‡è®¾ example ä¸­åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
    - "prompt": é¢˜å¹²å’Œé€‰é¡¹ (å­—ç¬¦ä¸²)
    - "label":  "A"/"B"/"C"/"D"
    - "exp":    å¯èƒ½ä¸ºç©ºï¼Œæˆ–è€…æ˜¯ä¸“å®¶æ€è·¯ (å­—ç¬¦ä¸²)

    å°†å®ƒä»¬ç»„åˆæˆä¸€æ®µ 'input_text'ï¼ŒåŒ…å«ï¼š
      - åŸå§‹ prompt
      - CoT æç¤º: "Let's think step by step."
      - å¦‚æœ exp ä¸ä¸ºç©ºï¼Œåˆ™åŠ å…¥ä¸“å®¶æ€è·¯
      - æœ€åé™„åŠ  "Answer: X"
    """
    text = example["prompt"]  # ä¾‹: "Question: ... \nOptions: ... \nAnswer:"
    label = example["label"]  # ä¾‹: "C"

    # æ‹¼æ¥ CoT æç¤º
    text += "\nLet's think step by step."

    # å¦‚æœ exp ä¸ä¸ºç©ºï¼ˆå¹¶ä¸”ä¸åªæ˜¯ç©ºæ ¼ï¼‰ï¼Œåˆ™åŠ ä¸Šä¸“å®¶æ€è·¯
    if (example.get("exp") is not None) and example["exp"].strip():
        text += f"\nExpert explanation: {example['exp']}"

    # æœ€åå†è¡¥ä¸Šâ€œAnswer: Xâ€
    text += f"\nAnswer: {label}"

    return {"input_text": text}


# 3) è®¾ç½® LoRA é…ç½®
lora_config = LoraConfig(
    r=8,  # adapter çš„ Rank
    lora_alpha=16,  # ç¼©æ”¾å› å­
    lora_dropout=0.1,  # Dropout æ¦‚ç‡
    task_type=TaskType.CAUSAL_LM,  # å› æœè¯­è¨€å»ºæ¨¡ä»»åŠ¡
)

# 4) æ„å»º LoRA æ¨¡å‹
model = get_peft_model(model, lora_config)
model.to("cuda")  # ç¡®ä¿æ¨¡å‹åœ¨ GPU ä¸Š
model.print_trainable_parameters()  # æ‰“å°å¯è®­ç»ƒå‚æ•°æ•°é‡

# 5) æ•°æ®åŠ è½½å™¨
train_dataset = train_dataset.map(format_example)
dev_dataset = dev_dataset.map(format_example)
train_dataset = train_dataset.filter(lambda x: x is not None and "input_text" in x)
dev_dataset = dev_dataset.filter(lambda x: x is not None and "input_text" in x)
train_subset = train_dataset.select(range(10000))  # æ„å»ºä¸€ä¸ª 10k çš„å­è®­ç»ƒé›†
dev_subset = dev_dataset.shuffle(seed=42).select(range(1000))  # å…ˆç”¨éªŒè¯é›†çš„ä¸€éƒ¨åˆ†è¿›è¡Œè®¡ç®—

from torch.utils.data._utils.collate import default_collate


def my_collate_fn(batch):
    for sample in batch:
        if sample is not None:
            # å°† None æ›¿æ¢ä¸ºé»˜è®¤å€¼ï¼Œä¾‹å¦‚ç©ºå­—ç¬¦ä¸²
            if sample.get("topic_name") is None:
                sample["topic_name"] = ""
            if sample.get("exp") is None:
                sample["exp"] = ""
    # è¿‡æ»¤æ‰æ•´ä½“ä¸º None çš„æ ·æœ¬
    filtered_batch = [sample for sample in batch if sample is not None]
    if len(filtered_batch) == 0:
        raise ValueError("è¿‡æ»¤åï¼Œå½“å‰æ‰¹æ¬¡æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬ï¼Œè¯·æ£€æŸ¥æ•°æ®é¢„å¤„ç†é€»è¾‘")
    return default_collate(filtered_batch)


train_dataloader = DataLoader(train_subset, batch_size=6, shuffle=True, collate_fn=my_collate_fn)
dev_dataloader = DataLoader(dev_subset, batch_size=6, collate_fn=my_collate_fn)

# 6) ä¼˜åŒ–å™¨
optimizer = AdamW(model.parameters(), lr=1e-4)

# 7) è®­ç»ƒå¾ªç¯è®¾ç½®
eval_interval = 300  # æ¯ 300 æ¬¡ä¼˜åŒ–æ­¥è¯„ä¼°ä¸€æ¬¡
epochs = 3
best_dev_loss = float("inf")  # ä¿å­˜å½“å‰æœ€å°çš„éªŒè¯é›†æŸå¤±
accumulation_steps = 3  # æ¯ 2 ä¸ª mini-batch ç´¯ç§¯ä¸€æ¬¡æ¢¯åº¦ï¼Œå®é™… batch_size ä¸º 8Ã—2=16
global_step = 0  # è®°å½•çœŸå®çš„ä¼˜åŒ–æ­¥æ•°

# åˆå§‹åŒ– AMP çš„ GradScaler
scaler = GradScaler()

for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()  # æ¯ä¸ª epoch å¼€å§‹æ—¶æ¢¯åº¦æ¸…é›¶

    for i, batch in enumerate(train_dataloader):
        # 1. å‡†å¤‡è¾“å…¥
        if "input_text" in batch:
            inputs = tokenizer(batch["input_text"], return_tensors="pt",
                               padding=True, truncation=True, max_length=1024).to("cuda")
        else:
            print("âŒ ç¼ºå¤± input_text çš„æ ·æœ¬ï¼š", batch)
            sys.exit("â›” ç¨‹åºå·²ç»ˆæ­¢ï¼Œå› ä¸ºæœ‰æ ·æœ¬ç¼ºå¤± input_text")

        # 2. å‰å‘ä¼ æ’­ï¼Œä½¿ç”¨ AMP è‡ªåŠ¨æ··åˆç²¾åº¦
        with autocast():
            labels = inputs.input_ids
            outputs = model(**inputs, labels=labels)
            loss = outputs.loss

        # 3. æ¢¯åº¦ç´¯ç§¯ï¼šåä¼  loss/accumulation_steps
        scaler.scale(loss / accumulation_steps).backward()

        # 4. å½“è¾¾åˆ°ç´¯ç§¯æ­¥æ•°åï¼Œè¿›è¡Œä¸€æ¬¡ä¼˜åŒ–æ›´æ–°
        if (i + 1) % accumulation_steps == 0:
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
            global_step += 1

            # 5. æ¯ eval_interval æ¬¡ä¼˜åŒ–æ­¥è¯„ä¼°ä¸€æ¬¡æ¨¡å‹
            if global_step % eval_interval == 0:
                model.eval()
                total_loss = 0
                with torch.no_grad():
                    for dev_batch in dev_dataloader:
                        dev_inputs = tokenizer(dev_batch["input_text"],
                                               return_tensors="pt", padding=True, truncation=True, max_length=1024).to(
                            "cuda")
                        dev_labels = dev_inputs.input_ids
                        with autocast():
                            dev_outputs = model(**dev_inputs, labels=dev_labels)
                        total_loss += dev_outputs.loss.item()
                avg_loss = total_loss / len(dev_dataloader)
                print(f"Epoch {epoch + 1}, Global Step {global_step}, Dev Loss: {avg_loss:.4f}")

                if avg_loss < best_dev_loss:
                    best_dev_loss = avg_loss
                    model.save_pretrained("/root/meditron-medmcqa-finetune/data/train_7/best")
                    print(f"ğŸ’¾ æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜ï¼Œå½“å‰ Dev Loss: {avg_loss:.4f}")
                torch.cuda.empty_cache()
                model.train()

    # æ¯ä¸ª epoch ç»“æŸåä¿å­˜ä¸€æ¬¡æ¨¡å‹
    save_path = f"/root/meditron-medmcqa-finetune/data/train_7/epoch_{epoch + 1}"
    model.save_pretrained(save_path)
    if epoch == 0:
        tokenizer.save_pretrained("/root/meditron-medmcqa-finetune/data/train_7/tokenizer")
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³ {save_path}")
